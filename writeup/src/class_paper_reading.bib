
@misc{ramesh_hierarchical_2022,
	title = {Hierarchical {Text}-{Conditional} {Image} {Generation} with {CLIP} {Latents}},
	url = {http://arxiv.org/abs/2204.06125},
	abstract = {Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, ﬁnding that the latter are computationally more efﬁcient and produce higher-quality samples.},
	language = {en},
	urldate = {2022-09-24},
	publisher = {arXiv},
	author = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
	month = apr,
	year = {2022},
	note = {arXiv:2204.06125 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Ramesh et al. - 2022 - Hierarchical Text-Conditional Image Generation wit.pdf:C\:\\Users\\ChekF\\Zotero\\storage\\225QIRV7\\Ramesh et al. - 2022 - Hierarchical Text-Conditional Image Generation wit.pdf:application/pdf},
}

@article{jaleel_high_nodate,
	title = {High {Performance} {Cache} {Replacement} {Using} {Re}-{Reference} {Interval} {Prediction} ({RRIP})},
	abstract = {Practical cache replacement policies attempt to emulate optimal replacement by predicting the re-reference interval of a cache block. The commonly used LRU replacement policy always predicts a nearimmediate re-reference interval on cache hits and misses. Applications that exhibit a distant re-reference interval perform badly under LRU. Such applications usually have a working-set larger than the cache or have frequent bursts of references to nontemporal data (called scans). To improve the performance of such workloads, this paper proposes cache replacement using Rereference Interval Prediction (RRIP). We propose Static RRIP (SRRIP) that is scan-resistant and Dynamic RRIP (DRRIP) that is both scan-resistant and thrash-resistant. Both RRIP policies require only 2-bits per cache block and easily integrate into existing LRU approximations found in modern processors. Our evaluations using PC games, multimedia, server and SPEC CPU2006 workloads on a single-core processor with a 2MB last-level cache (LLC) show that both SRRIP and DRRIP outperform LRU replacement on the throughput metric by an average of 4\% and 10\% respectively. Our evaluations with over 1000 multi-programmed workloads on a 4-core CMP with an 8MB shared LLC show that SRRIP and DRRIP outperform LRU replacement on the throughput metric by an average of 7\% and 9\% respectively. We also show that RRIP outperforms LFU, the state-of the art scan-resistant replacement algorithm todate. For the cache configurations under study, RRIP requires 2X less hardware than LRU and 2.5X less hardware than LFU.},
	language = {en},
	author = {Jaleel, Aamer and Theobald, Kevin B and Jr, Simon C Steely and Emer, Joel},
	pages = {12},
	file = {Jaleel et al. - High Performance Cache Replacement Using Re-Refere.pdf:C\:\\Users\\ChekF\\Zotero\\storage\\U8GRWB9A\\Jaleel et al. - High Performance Cache Replacement Using Re-Refere.pdf:application/pdf},
}

@article{sherwood_automatically_nodate,
	title = {Automatically {Characterizing} {Large} {Scale} {Program} {Behavior}},
	abstract = {Understanding program behavior is at the foundation of computer architecture and program optimization. Many programs have wildly diﬀerent behavior on even the very largest of scales (over the complete execution of the program). This realization has ramiﬁcations for many architectural and compiler techniques, from thread scheduling, to feedback directed optimizations, to the way programs are simulated. However, in order to take advantage of time-varying behavior, we must ﬁrst develop the analytical tools necessary to automatically and eﬃciently analyze program behavior over large sections of execution.},
	language = {en},
	author = {Sherwood, Timothy and Perelman, Erez and Hamerly, Greg and Calder, Brad},
	pages = {13},
	file = {Sherwood et al. - Automatically Characterizing Large Scale Program B.pdf:C\:\\Users\\ChekF\\Zotero\\storage\\FFQKST7I\\Sherwood et al. - Automatically Characterizing Large Scale Program B.pdf:application/pdf},
}

@inproceedings{fowers_configurable_2018,
	address = {Los Angeles, CA},
	title = {A {Configurable} {Cloud}-{Scale} {DNN} {Processor} for {Real}-{Time} {AI}},
	isbn = {978-1-5386-5984-7},
	url = {https://ieeexplore.ieee.org/document/8416814/},
	doi = {10.1109/ISCA.2018.00012},
	abstract = {Interactive AI-powered services require low-latency evaluation of deep neural network (DNN) models—aka “realtime AI”. The growing demand for computationally expensive, state-of-the-art DNNs, coupled with diminishing performance gains of general-purpose architectures, has fueled an explosion of specialized Neural Processing Units (NPUs). NPUs for interactive services should satisfy two requirements: (1) execution of DNN models with low latency, high throughput, and high efﬁciency, and (2) ﬂexibility to accommodate evolving state-of-the-art models (e.g., RNNs, CNNs, MLPs) without costly silicon updates.},
	language = {en},
	urldate = {2022-10-21},
	booktitle = {2018 {ACM}/{IEEE} 45th {Annual} {International} {Symposium} on {Computer} {Architecture} ({ISCA})},
	publisher = {IEEE},
	author = {Fowers, Jeremy and Ovtcharov, Kalin and Papamichael, Michael and Massengill, Todd and Liu, Ming and Lo, Daniel and Alkalay, Shlomi and Haselman, Michael and Adams, Logan and Ghandi, Mahdi and Heil, Stephen and Patel, Prerak and Sapek, Adam and Weisz, Gabriel and Woods, Lisa and Lanka, Sitaram and Reinhardt, Steven K. and Caulfield, Adrian M. and Chung, Eric S. and Burger, Doug},
	month = jun,
	year = {2018},
	pages = {1--14},
	file = {Fowers et al. - 2018 - A Configurable Cloud-Scale DNN Processor for Real-.pdf:C\:\\Users\\ChekF\\Zotero\\storage\\BJIL5D8L\\Fowers et al. - 2018 - A Configurable Cloud-Scale DNN Processor for Real-.pdf:application/pdf},
}

@inproceedings{jimenez_dynamic_2001,
	address = {Monterrey, Mexico},
	title = {Dynamic branch prediction with perceptrons},
	isbn = {978-0-7695-1019-4},
	url = {http://ieeexplore.ieee.org/document/903263/},
	doi = {10.1109/HPCA.2001.903263},
	abstract = {This paper presents a new method for branch prediction. The key idea is to use one of the simplest possible neural networks, the perceptron as an alternative to the commonly used two-bit counters. Our predictor achieves increased accuracy by making use of long branch histories, which are possible because the hardware resources for our method scale linearly with the history length. By contrast, other purely dynamic schemes require exponential resources.},
	language = {en},
	urldate = {2023-01-15},
	booktitle = {Proceedings {HPCA} {Seventh} {International} {Symposium} on {High}-{Performance} {Computer} {Architecture}},
	publisher = {IEEE Comput. Soc},
	author = {Jimenez, D.A. and Lin, C.},
	year = {2001},
	pages = {197--206},
	file = {Jimenez and Lin - 2001 - Dynamic branch prediction with perceptrons.pdf:C\:\\Users\\ChekF\\Zotero\\storage\\55UMN9DW\\Jimenez and Lin - 2001 - Dynamic branch prediction with perceptrons.pdf:application/pdf},
}

@article{ho_future_2001,
	title = {The future of wires},
	volume = {89},
	issn = {00189219},
	url = {http://ieeexplore.ieee.org/document/920580/},
	doi = {10.1109/5.920580},
	language = {en},
	number = {4},
	urldate = {2023-01-17},
	journal = {Proceedings of the IEEE},
	author = {Ho, R. and Mai, K.W. and Horowitz, M.A.},
	month = apr,
	year = {2001},
	pages = {490--504},
	file = {Ho et al. - 2001 - The future of wires.pdf:C\:\\Users\\ChekF\\Zotero\\storage\\6ZRDPZEH\\Ho et al. - 2001 - The future of wires.pdf:application/pdf},
}

@article{poulton_054_2013,
	title = {A 0.54 {pJ}/b 20 {Gb}/s {Ground}-{Referenced} {Single}-{Ended} {Short}-{Reach} {Serial} {Link} in 28 nm {CMOS} for {Advanced} {Packaging} {Applications}},
	volume = {48},
	issn = {0018-9200, 1558-173X},
	url = {http://ieeexplore.ieee.org/document/6601723/},
	doi = {10.1109/JSSC.2013.2279053},
	abstract = {High-speed signaling over high density interconnect on organic package substrates or silicon interposers offers an attractive solution to the off-chip bandwidth limitation problem faced in modern digital systems. In this paper, we describe a signaling system co-designed with the interconnect to take advantage of the characteristics of this environment to enable a high-speed, low area, and low-power die to die link. Ground-Referenced Signaling (GRS) is a single-ended signaling system that eliminates the major problems traditionally associated with single-ended design by using the ground plane as the reference and signaling above and below ground. This design employs a novel charge pump driver that additionally eliminates the issue of simultaneous switching noise with data independent current consumption. Silicon measurements from a test chip implementing two 16-lane links, with forwarded clocks, in a standard 28 nm process demonstrate 20 Gb/s operation at 0.54 pJ/bit over 4.5 mm organic substrate channels at a nominal 0.9 V power supply voltage. Timing margins at the receiver are 0.3 UI at a BER of 10 . We estimate BER 10 at the eye center.},
	language = {en},
	number = {12},
	urldate = {2023-01-17},
	journal = {IEEE Journal of Solid-State Circuits},
	author = {Poulton, John W. and Dally, William J. and Chen, Xi and Eyles, John G. and Greer, Thomas H. and Tell, Stephen G. and Wilson, John M. and Gray, C. Thomas},
	month = dec,
	year = {2013},
	pages = {3206--3218},
	file = {Poulton et al. - 2013 - A 0.54 pJb 20 Gbs Ground-Referenced Single-Ended.pdf:C\:\\Users\\ChekF\\Zotero\\storage\\DFI79ZIU\\Poulton et al. - 2013 - A 0.54 pJb 20 Gbs Ground-Referenced Single-Ended.pdf:application/pdf},
}

@article{jaleel_high_nodate-1,
	title = {High {Performance} {Cache} {Replacement} {Using} {Re}-{Reference} {Interval} {Prediction} ({RRIP})},
	abstract = {Practical cache replacement policies attempt to emulate optimal replacement by predicting the re-reference interval of a cache block. The commonly used LRU replacement policy always predicts a nearimmediate re-reference interval on cache hits and misses. Applications that exhibit a distant re-reference interval perform badly under LRU. Such applications usually have a working-set larger than the cache or have frequent bursts of references to nontemporal data (called scans). To improve the performance of such workloads, this paper proposes cache replacement using Rereference Interval Prediction (RRIP). We propose Static RRIP (SRRIP) that is scan-resistant and Dynamic RRIP (DRRIP) that is both scan-resistant and thrash-resistant. Both RRIP policies require only 2-bits per cache block and easily integrate into existing LRU approximations found in modern processors. Our evaluations using PC games, multimedia, server and SPEC CPU2006 workloads on a single-core processor with a 2MB last-level cache (LLC) show that both SRRIP and DRRIP outperform LRU replacement on the throughput metric by an average of 4\% and 10\% respectively. Our evaluations with over 1000 multi-programmed workloads on a 4-core CMP with an 8MB shared LLC show that SRRIP and DRRIP outperform LRU replacement on the throughput metric by an average of 7\% and 9\% respectively. We also show that RRIP outperforms LFU, the state-of the art scan-resistant replacement algorithm todate. For the cache configurations under study, RRIP requires 2X less hardware than LRU and 2.5X less hardware than LFU.},
	language = {en},
	author = {Jaleel, Aamer and Theobald, Kevin B and Jr, Simon C Steely and Emer, Joel},
	file = {Jaleel et al. - High Performance Cache Replacement Using Re-Refere.pdf:C\:\\Users\\ChekF\\Zotero\\storage\\XSZ723NY\\Jaleel et al. - High Performance Cache Replacement Using Re-Refere.pdf:application/pdf},
}

@article{qureshi_adaptive_nodate,
	title = {Adaptive {Insertion} {Policies} for {High} {Performance} {Caching}},
	abstract = {The commonly used LRU replacement policy is susceptible to thrashing for memory-intensive workloads that have a working set greater than the available cache size. For such applications, the majority of lines traverse from the MRU position to the LRU position without receiving any cache hits, resulting in inefﬁcient use of cache space. Cache performance can be improved if some fraction of the working set is retained in the cache so that at least that fraction of the working set can contribute to cache hits.},
	language = {en},
	author = {Qureshi, Moinuddin K and Jaleel, Aamer and Patt, Yale N and Jr, Simon C Steely and Emer, Joel},
	file = {Qureshi et al. - Adaptive Insertion Policies for High Performance C.pdf:C\:\\Users\\ChekF\\Zotero\\storage\\B6TRBZX2\\Qureshi et al. - Adaptive Insertion Policies for High Performance C.pdf:application/pdf},
}

@article{bolotin_designing_2015,
	title = {Designing {Efficient} {Heterogeneous} {Memory} {Architectures}},
	volume = {35},
	issn = {1937-4143},
	doi = {10.1109/MM.2015.72},
	abstract = {Recent packaging technologies that enable DRAM chips to be stacked inside the processor package or on top of the processor chip can lower DRAM energy-per-bit costs, provide wider interfaces, and offer higher bandwidth. However, these technologies are limited in capacity and come at a higher price than traditional off-package memories, requiring system designers to balance price, performance, and capacity tradeoffs. The most obvious means to achieve this balance is to employ both on- and off-package memory in a heterogeneous memory architecture. However, designers must then decide whether to deploy the on-package memory as an additional cache-hierarchy level (controlled by hardware or software) or as a memory peer to the off-package DRAM in a NUMA configuration. This article presents a model and analysis of energy, bandwidth, and latency for current and emerging DRAM technologies that enable an exploration of memory hierarchies combining heterogeneous memory technologies with different attributes. The analysis shows that the gap between on- and off-package DRAM technologies is narrower than what is found between cache layers in traditional memory hierarchies. As a result, heterogeneous memory caches must achieve very high hit rates or risk degrading both system energy and bandwidth efficiency.},
	number = {4},
	journal = {IEEE Micro},
	author = {Bolotin, Evgeny and Nellans, David and Villa, Oreste and O'Connor, Mike and Ramirez, Alex and Keckler, Stephen W.},
	month = jul,
	year = {2015},
	note = {Conference Name: IEEE Micro},
	keywords = {Bandwidth, Random access memory, Analytical models, cache hierarchy, Computer programs, DRAM, energy efficiency, hardware, heterogeneous memory architecture, Memory architecture, software, Software engineering},
	pages = {60--68},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\ChekF\\Zotero\\storage\\QXDV72WC\\7155441.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\ChekF\\Zotero\\storage\\MFXIFQQ8\\Bolotin et al. - 2015 - Designing Efficient Heterogeneous Memory Architect.pdf:application/pdf},
}

@inproceedings{wu_ship_2011,
	address = {Porto Alegre Brazil},
	title = {{SHiP}: signature-based hit predictor for high performance caching},
	isbn = {978-1-4503-1053-6},
	shorttitle = {{SHiP}},
	url = {https://dl.acm.org/doi/10.1145/2155620.2155671},
	doi = {10.1145/2155620.2155671},
	abstract = {The shared last-level caches in CMPs play an important role in improving application performance and reducing off-chip memory bandwidth requirements. In order to use LLCs more efﬁciently, recent research has shown that changing the re-reference prediction on cache insertions and cache hits can signiﬁcantly improve cache performance. A fundamental challenge, however, is how to best predict the re-reference pattern of an incoming cache line.},
	language = {en},
	urldate = {2023-01-29},
	booktitle = {Proceedings of the 44th {Annual} {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture}},
	publisher = {ACM},
	author = {Wu, Carole-Jean and Jaleel, Aamer and Hasenplaugh, Will and Martonosi, Margaret and Steely, Simon C. and Emer, Joel},
	month = dec,
	year = {2011},
	pages = {430--441},
	file = {Wu et al. - 2011 - SHiP signature-based hit predictor for high perfo.pdf:C\:\\Users\\ChekF\\Zotero\\storage\\HYXE92GP\\Wu et al. - 2011 - SHiP signature-based hit predictor for high perfo.pdf:application/pdf},
}

@article{jain_back_nodate,
	title = {Back to the {Future}: {Leveraging} {Belady}’s {Algorithm} for {Improved} {Cache} {Replacement}},
	abstract = {Belady’s algorithm is optimal but infeasible because it requires knowledge of the future. This paper explains how a cache replacement algorithm can nonetheless learn from Belady’s algorithm by applying it to past cache accesses to inform future cache replacement decisions. We show that the implementation is surprisingly efﬁcient, as we introduce a new method of efﬁciently simulating Belady’s behavior, and we use known sampling techniques to compactly represent the long history information that is needed for high accuracy. For a 2MB LLC, our solution uses a 16KB hardware budget (excluding replacement state in the tag array). When applied to a memory-intensive subset of the SPEC 2006 CPU benchmarks, our solution improves performance over LRU by 8.4\%, as opposed to 6.2\% for the previous state-of-theart. For a 4-core system with a shared 8MB LLC, our solution improves performance by 15.0\%, compared to 12.0\% for the previous state-of-the-art.},
	language = {en},
	author = {Jain, Akanksha and Lin, Calvin},
	file = {Jain and Lin - Back to the Future Leveraging Belady’s Algorithm .pdf:C\:\\Users\\ChekF\\Zotero\\storage\\D9QE3KN2\\Jain and Lin - Back to the Future Leveraging Belady’s Algorithm .pdf:application/pdf},
}

@inproceedings{pugsley_sandbox_2014,
	address = {Orlando, FL, USA},
	title = {Sandbox {Prefetching}: {Safe} run-time evaluation of aggressive prefetchers},
	isbn = {978-1-4799-3097-5},
	shorttitle = {Sandbox {Prefetching}},
	url = {http://ieeexplore.ieee.org/document/6835971/},
	doi = {10.1109/HPCA.2014.6835971},
	abstract = {Memory latency is a major factor in limiting CPU performance, and prefetching is a well-known method for hiding memory latency. Overly aggressive prefetching can waste scarce resources such as memory bandwidth and cache capacity, limiting or even hurting performance. It is therefore important to employ prefetching mechanisms that use these resources prudently, while still prefetching required data in a timely manner.},
	language = {en},
	urldate = {2023-02-05},
	booktitle = {2014 {IEEE} 20th {International} {Symposium} on {High} {Performance} {Computer} {Architecture} ({HPCA})},
	publisher = {IEEE},
	author = {Pugsley, Seth H and Chishti, Zeshan and Wilkerson, Chris and Chuang, Peng-fei and Scott, Robert L and Jaleel, Aamer and Lu, Shih-Lien and Chow, Kingsum and Balasubramonian, Rajeev},
	month = feb,
	year = {2014},
	pages = {626--637},
	file = {Pugsley et al. - 2014 - Sandbox Prefetching Safe run-time evaluation of a.pdf:C\:\\Users\\ChekF\\Zotero\\storage\\K2T23BQK\\Pugsley et al. - 2014 - Sandbox Prefetching Safe run-time evaluation of a.pdf:application/pdf},
}

@inproceedings{kim_path_2016,
	address = {Taipei},
	title = {Path confidence based lookahead prefetching},
	isbn = {978-1-5090-3508-3},
	url = {https://ieeexplore.ieee.org/document/7783763/},
	doi = {10.1109/MICRO.2016.7783763},
	abstract = {Designing prefetchers to maximize system performance often requires a delicate balance between coverage and accuracy. Achieving both high coverage and accuracy is particularly challenging in workloads with complex address patterns, which may require large amounts of history to accurately predict future addresses. This paper describes the Signature Path Prefetcher (SPP), which offers effective solutions for three classic challenges in prefetcher design. First, SPP uses a compressed history based scheme that accurately predicts complex address patterns. Second, unlike other history based algorithms, which miss out on many prefetching opportunities when address patterns make a transition between physical pages, SPP tracks complex patterns across physical page boundaries and continues prefetching as soon as they move to new pages. Finally, SPP uses the conﬁdence it has in its predictions to adaptively throttle itself on a perprefetch stream basis. In our analysis, we ﬁnd that SPP improves performance by 27.2\% over a no-prefetching baseline, and outperforms the state-of-the-art Best Offset prefetcher by 6.4\%. SPP does this with minimal overhead, operating strictly in the physical address space, and without requiring any additional processor core state, such as the PC.},
	language = {en},
	urldate = {2023-02-05},
	booktitle = {2016 49th {Annual} {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture} ({MICRO})},
	publisher = {IEEE},
	author = {Kim, Jinchun and Pugsley, Seth H. and Gratz, Paul V. and Reddy, A.L. Narasimha and Wilkerson, Chris and Chishti, Zeshan},
	month = oct,
	year = {2016},
	pages = {1--12},
	file = {Kim et al. - 2016 - Path confidence based lookahead prefetching.pdf:C\:\\Users\\ChekF\\Zotero\\storage\\CG6V4A7E\\Kim et al. - 2016 - Path confidence based lookahead prefetching.pdf:application/pdf},
}

@article{spranglelt_agree_nodate,
	title = {The {Agree} {Predictor}: {A} {Mechanism} for {Reducing} {Negative} {Branch} {History} {Interference}},
	abstract = {Deeply pipelined, superscalar processors require accurate branch prediction to achieve high performance. Two-level branch predictors have been shown to achieve high prediction accuracy. It has also been shown that branch interference is a major contributor t o the number of branches mispredicted by two-level predictors.},
	language = {en},
	author = {Spranglelt, Eric and Chappelltt, Robert S and Alsupt, Mitch},
	file = {Spranglelt et al. - The Agree Predictor A Mechanism for Reducing Nega.pdf:C\:\\Users\\ChekF\\Zotero\\storage\\K7NJGLB2\\Spranglelt et al. - The Agree Predictor A Mechanism for Reducing Nega.pdf:application/pdf},
}

@inproceedings{jimenez_fast_2003,
	address = {San Diego, CA, USA},
	title = {Fast path-based neural branch prediction},
	isbn = {978-0-7695-2043-8},
	url = {http://ieeexplore.ieee.org/document/1253199/},
	doi = {10.1109/MICRO.2003.1253199},
	abstract = {Microarchitectural prediction based on neural learning has received increasing attention in recent years. However, neural prediction remains impractical because its superior accuracy over conventional predictors is not enough to offset the cost imposed by its high latency. We present a new neural branch predictor that solves the problem from both directions: it is both more accurate and much faster than previous neural predictors. Our predictor improves accuracy by combining path and pattern history to overcome limitations inherent to previous predictors. It also has much lower latency than previous neural predictors. The result is a predictor with accuracy far superior to conventional predictors but with latency comparable to predictors from industrial designs. Our simulations show that a path-based neural predictor improves the instructions-per-cycle (IPC) rate of an aggressively clocked microarchitecture by 16\% over the original perceptron predictor.},
	language = {en},
	urldate = {2023-02-09},
	booktitle = {22nd {Digital} {Avionics} {Systems} {Conference}. {Proceedings} ({Cat}. {No}.{03CH37449})},
	publisher = {IEEE Comput. Soc},
	author = {Jimenez, D.A.},
	year = {2003},
	pages = {243--252},
	file = {Jimenez - 2003 - Fast path-based neural branch prediction.pdf:C\:\\Users\\ChekF\\Zotero\\storage\\CRJ2CV95\\Jimenez - 2003 - Fast path-based neural branch prediction.pdf:application/pdf},
}

@inproceedings{nesbit_data_2004,
	address = {Madrid, Spain},
	title = {Data {Cache} {Prefetching} {Using} a {Global} {History} {Buffer}},
	isbn = {978-0-7695-2053-7},
	url = {http://ieeexplore.ieee.org/document/1410068/},
	doi = {10.1109/HPCA.2004.10030},
	abstract = {A new structure for implementing data cache prefetching is proposed and analyzed via simulation. The structure is based on a Global History Buffer that holds the most recent miss addresses in FIFO order. Linked lists within this global history buffer connect addresses that have some common property, e.g. they were all generated by the same load instruction. The Global History Buffer can be used for implementing a number of previously proposed prefetch methods, as well as new ones.},
	language = {en},
	urldate = {2023-02-13},
	booktitle = {10th {International} {Symposium} on {High} {Performance} {Computer} {Architecture} ({HPCA}'04)},
	publisher = {IEEE},
	author = {Nesbit, K.J. and Smith, J.E.},
	year = {2004},
	pages = {96--96},
	file = {Nesbit and Smith - 2004 - Data Cache Prefetching Using a Global History Buff.pdf:C\:\\Users\\ChekF\\Zotero\\storage\\4BQ3G37A\\Nesbit and Smith - 2004 - Data Cache Prefetching Using a Global History Buff.pdf:application/pdf},
}

@inproceedings{jain_linearizing_2013,
	address = {Davis California},
	title = {Linearizing irregular memory accesses for improved correlated prefetching},
	isbn = {978-1-4503-2638-4},
	url = {https://dl.acm.org/doi/10.1145/2540708.2540730},
	doi = {10.1145/2540708.2540730},
	abstract = {This paper introduces the Irregular Stream Buﬀer (ISB), a prefetcher that targets irregular sequences of temporally correlated memory references. The key idea is to use an extra level of indirection to translate arbitrary pairs of correlated physical addresses into consecutive addresses in a new structural address space, which is visible only to the ISB. This structural address space allows the ISB to organize prefetching meta-data so that it is simultaneously temporally and spatially ordered, which produces technical beneﬁts in terms of coverage, accuracy, and memory traﬃc overhead.},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {Proceedings of the 46th {Annual} {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture}},
	publisher = {ACM},
	author = {Jain, Akanksha and Lin, Calvin},
	month = dec,
	year = {2013},
	pages = {247--259},
	file = {Jain and Lin - 2013 - Linearizing irregular memory accesses for improved.pdf:C\:\\Users\\ChekF\\Zotero\\storage\\U6CL4NXW\\Jain and Lin - 2013 - Linearizing irregular memory accesses for improved.pdf:application/pdf},
}

@inproceedings{wenisch_practical_2009,
	address = {Raleigh, NC, USA},
	title = {Practical off-chip meta-data for temporal memory streaming},
	isbn = {978-1-4244-2932-5},
	url = {http://ieeexplore.ieee.org/document/4798239/},
	doi = {10.1109/HPCA.2009.4798239},
	abstract = {Prior research demonstrates that temporal memory streaming and related address-correlating prefetchers improve performance of commercial server workloads though increased memory level parallelism. Unfortunately, these prefetchers require large on-chip meta-data storage, making previously-proposed designs impractical. Hence, to improve practicality, researchers have sought ways to enable timely prefetch while locating meta-data entirely off-chip. Unfortunately, current solutions for off-chip meta-data increase memory traffic by over a factor of three.},
	language = {en},
	urldate = {2023-02-18},
	booktitle = {2009 {IEEE} 15th {International} {Symposium} on {High} {Performance} {Computer} {Architecture}},
	publisher = {IEEE},
	author = {Wenisch, Thomas F. and Ferdman, Michael and Ailamaki, Anastasia and Falsafi, Babak and Moshovos, Andreas},
	month = feb,
	year = {2009},
	pages = {79--90},
	file = {Wenisch et al. - 2009 - Practical off-chip meta-data for temporal memory s.pdf:C\:\\Users\\ChekF\\Zotero\\storage\\KZ7GAEY7\\Wenisch et al. - 2009 - Practical off-chip meta-data for temporal memory s.pdf:application/pdf},
}

@misc{zhang_multimodal_2023,
	title = {Multimodal {Chain}-of-{Thought} {Reasoning} in {Language} {Models}},
	url = {http://arxiv.org/abs/2302.00923},
	abstract = {Large language models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer. However, existing CoT studies have focused on the language modality. We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference. In this way, answer inference can leverage better generated rationales that are based on multimodal information. With Multimodal-CoT, our model under 1 billion parameters outperforms the previous state-of-the-art LLM (GPT-3.5) by 16 percentage points (75.17\%-{\textgreater}91.68\% accuracy) on the ScienceQA benchmark and even surpasses human performance. Code is publicly available available at https://github.com/amazon-science/mm-cot.},
	language = {en},
	urldate = {2023-02-21},
	publisher = {arXiv},
	author = {Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Zhao, Hai and Karypis, George and Smola, Alex},
	month = feb,
	year = {2023},
	note = {arXiv:2302.00923 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {Zhang et al. - 2023 - Multimodal Chain-of-Thought Reasoning in Language .pdf:C\:\\Users\\ChekF\\Zotero\\storage\\ENWJXBT7\\Zhang et al. - 2023 - Multimodal Chain-of-Thought Reasoning in Language .pdf:application/pdf},
}

@inproceedings{enright_jerger_friendly_2006,
	address = {Austin, TX, USA},
	title = {Friendly fire: understanding the effects of multiprocessor prefetches},
	isbn = {978-1-4244-0186-4},
	shorttitle = {Friendly fire},
	url = {http://ieeexplore.ieee.org/document/1620802/},
	doi = {10.1109/ISPASS.2006.1620802},
	abstract = {Modern processors attempt to overcome increasing memory latencies by anticipating future references and prefetching those blocks from memory. The behavior and possible negative side effects of prefetching schemes are fairly well understood for uniprocessor systems. However, in a multiprocessor system a prefetch can steal read and/or write permissions for shared blocks from other processors, leading to permission thrashing and overall performance degradation. In this paper, we present a taxonomy that classifies the effects of multiprocessor prefetches. We also present a characterization of the effects of four different hardware prefetching schemes--sequential prefetching, content-directed data prefetching, wrong path prefetching and exclusive prefetching--in a bus-based multiprocessor system. We show that accuracy and coverage are inadequate metrics for describing prefetching in a multiprocessor; rather, we also need to understand what fraction of prefetches interfere with remote processors. We present an upper bound on the performance of various prefetching algorithms if no harmful prefetches are issued, and suggest prefetch filtering schemes that can accomplish this goal.},
	language = {en},
	urldate = {2023-02-25},
	booktitle = {2006 {IEEE} {International} {Symposium} on {Performance} {Analysis} of {Systems} and {Software}},
	publisher = {IEEE},
	author = {Enright Jerger, N.D. and Hill, E.L. and Lipasti, M.H.},
	year = {2006},
	pages = {177--188},
	file = {Enright Jerger et al. - 2006 - Friendly fire understanding the effects of multip.pdf:C\:\\Users\\ChekF\\Zotero\\storage\\GA273FUL\\Enright Jerger et al. - 2006 - Friendly fire understanding the effects of multip.pdf:application/pdf},
}

@inproceedings{mukherjee_using_nodate,
author = {Mukherjee, Shubhendu S. and Hill, Mark D.},
title = {Using Prediction to Accelerate Coherence Protocols},
year = {1998},
isbn = {0818684917},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1145/279358.279386},
doi = {10.1145/279358.279386},
abstract = {Most large shared-memory multiprocessors use directory protocols to keep per-processor caches coherent. Some memory references in such systems, however, suffer long latencies for misses to remotely-cached blocks. To ameliorate this latency, researchers have augmented standard coherence protocols with optimizations for specific sharing patterns, such as read-modify-write, producer-consumer, and migratory sharing. This paper seeks to replace these directed solutions with general prediction logic that monitors coherence activity and triggers appropriate coherence actions.This paper takes the first step toward using general prediction to accelerate coherence protocols by developing and evaluating the Cosmos coherence message predictor. Cosmos predicts the source and type of the next coherence message for a cache block using logic that is an extension of Yeh and Patt's two-level PAp branch predictor. For five scientific applications running on 16 processors, Cosmos has prediction accuracies of 62\% to 93\%. Cosmos' high prediction accuracy is a result of predictable coherence message signatures that arise from stable sharing patterns of cache blocks.},
booktitle = {Proceedings of the 25th Annual International Symposium on Computer Architecture},
pages = {179–190},
numpages = {12},
location = {Barcelona, Spain},
series = {ISCA '98}
}

@misc{bera_hermes_2022,
	title = {Hermes: {Accelerating} {Long}-{Latency} {Load} {Requests} via {Perceptron}-{Based} {Off}-{Chip} {Load} {Prediction}},
	shorttitle = {Hermes},
	url = {http://arxiv.org/abs/2209.00188},
	abstract = {Long-latency load requests continue to limit the performance of high-performance processors. To increase the latency tolerance of a processor, architects have primarily relied on two key techniques: sophisticated data prefetchers and large on-chip caches. In this work, we show that: 1) even a sophisticated state-of-the-art prefetcher can only predict half of the off-chip load requests on average across a wide range of workloads, and 2) due to the increasing size and complexity of on-chip caches, a large fraction of the latency of an off-chip load request is spent accessing the on-chip cache hierarchy. The goal of this work is to accelerate off-chip load requests by removing the on-chip cache access latency from their critical path. To this end, we propose a new technique called Hermes, whose key idea is to: 1) accurately predict which load requests might go off-chip, and 2) speculatively fetch the data required by the predicted off-chip loads directly from the main memory, while also concurrently accessing the cache hierarchy for such loads. To enable Hermes, we develop a new lightweight, perceptron-based off-chip load prediction technique that learns to identify off-chip load requests using multiple program features (e.g., sequence of program counters). For every load request, the predictor observes a set of program features to predict whether or not the load would go off-chip. If the load is predicted to go off-chip, Hermes issues a speculative request directly to the memory controller once the load's physical address is generated. If the prediction is correct, the load eventually misses the cache hierarchy and waits for the ongoing speculative request to finish, thus hiding the on-chip cache hierarchy access latency from the critical path of the off-chip load. Our evaluation shows that Hermes significantly improves performance of a state-of-the-art baseline. We open-source Hermes.},
	language = {en},
	urldate = {2023-03-04},
	publisher = {arXiv},
	author = {Bera, Rahul and Kanellopoulos, Konstantinos and Balachandran, Shankar and Novo, David and Olgun, Ataberk and Sadrosadati, Mohammad and Mutlu, Onur},
	month = sep,
	year = {2022},
	note = {arXiv:2209.00188 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Hardware Architecture, B.3.2, C.0},
	file = {Bera et al. - 2022 - Hermes Accelerating Long-Latency Load Requests vi.pdf:C\:\\Users\\ChekF\\Zotero\\storage\\W99K5WLW\\Bera et al. - 2022 - Hermes Accelerating Long-Latency Load Requests vi.pdf:application/pdf},
}

@inproceedings{ferdman_temporal_2008,
	address = {Como, Italy},
	title = {Temporal instruction fetch streaming},
	isbn = {978-1-4244-2836-6},
	url = {http://ieeexplore.ieee.org/document/4771774/},
	doi = {10.1109/MICRO.2008.4771774},
	abstract = {L1 instruction-cache misses pose a critical performance bottleneck in commercial server workloads. Cache access latency constraints preclude L1 instruction caches large enough to capture the application, library, and OS instruction working sets of these workloads. To cope with capacity constraints, researchers have proposed instruction prefetchers that use branch predictors to explore future control flow. However, such prefetchers suffer from several fundamental flaws: their lookahead is limited by branch prediction bandwidth, their accuracy suffers from geometrically-compounding branch misprediction probability, and they are ignorant of the cache contents, frequently predicting blocks already present in L1. Hence, L1 instruction misses remain a bottleneck.},
	language = {en},
	urldate = {2023-03-04},
	booktitle = {2008 41st {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture}},
	publisher = {IEEE},
	author = {Ferdman, Michael and Wenisch, Thomas F. and Ailamaki, Anastasia and Falsafi, Babak and Moshovos, Andreas},
	month = nov,
	year = {2008},
	pages = {1--10},
	file = {Ferdman et al. - 2008 - Temporal instruction fetch streaming.pdf:C\:\\Users\\ChekF\\Zotero\\storage\\X8IC3PCE\\Ferdman et al. - 2008 - Temporal instruction fetch streaming.pdf:application/pdf},
}

@misc{ouyang_training_2022,
	title = {Training language models to follow instructions with human feedback},
	url = {http://arxiv.org/abs/2203.02155},
	abstract = {Making language models bigger does not inherently make them better at following a user’s intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by ﬁne-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to ﬁne-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further ﬁne-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that ﬁne-tuning with human feedback is a promising direction for aligning language models with human intent.},
	language = {en},
	urldate = {2023-03-07},
	publisher = {arXiv},
	author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
	month = mar,
	year = {2022},
	note = {arXiv:2203.02155 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Ouyang et al. - 2022 - Training language models to follow instructions wi.pdf:C\:\\Users\\ChekF\\Zotero\\storage\\668V7VSL\\2203.02155.pdf:application/pdf},
}

@misc{adiwardana_towards_2020,
	title = {Towards a {Human}-like {Open}-{Domain} {Chatbot}},
	url = {http://arxiv.org/abs/2001.09977},
	doi = {10.48550/arXiv.2001.09977},
	abstract = {We present Meena, a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is simply trained to minimize perplexity of the next token. We also propose a human evaluation metric called Sensibleness and Specificity Average (SSA), which captures key elements of a human-like multi-turn conversation. Our experiments show strong correlation between perplexity and SSA. The fact that the best perplexity end-to-end trained Meena scores high on SSA (72\% on multi-turn evaluation) suggests that a human-level SSA of 86\% is potentially within reach if we can better optimize perplexity. Additionally, the full version of Meena (with a filtering mechanism and tuned decoding) scores 79\% SSA, 23\% higher in absolute SSA than the existing chatbots we evaluated.},
	urldate = {2023-03-07},
	publisher = {arXiv},
	author = {Adiwardana, Daniel and Luong, Minh-Thang and So, David R. and Hall, Jamie and Fiedel, Noah and Thoppilan, Romal and Yang, Zi and Kulshreshtha, Apoorv and Nemade, Gaurav and Lu, Yifeng and Le, Quoc V.},
	month = feb,
	year = {2020},
	note = {arXiv:2001.09977 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\ChekF\\Zotero\\storage\\4Y26HKPR\\Adiwardana et al. - 2020 - Towards a Human-like Open-Domain Chatbot.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ChekF\\Zotero\\storage\\XVIZGUYX\\2001.html:text/html},
}

@misc{wei_finetuned_2022,
	title = {Finetuned {Language} {Models} {Are} {Zero}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2109.01652},
	doi = {10.48550/arXiv.2109.01652},
	abstract = {This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.},
	urldate = {2023-03-07},
	publisher = {arXiv},
	author = {Wei, Jason and Bosma, Maarten and Zhao, Vincent Y. and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M. and Le, Quoc V.},
	month = feb,
	year = {2022},
	note = {arXiv:2109.01652 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\ChekF\\Zotero\\storage\\BN3WS382\\Wei et al. - 2022 - Finetuned Language Models Are Zero-Shot Learners.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ChekF\\Zotero\\storage\\2JUVJXBS\\2109.html:text/html},
}

@misc{kojima_large_2023,
	title = {Large {Language} {Models} are {Zero}-{Shot} {Reasoners}},
	url = {http://arxiv.org/abs/2205.11916},
	doi = {10.48550/arXiv.2205.11916},
	abstract = {Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding "Let's think step by step" before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7\% to 78.7\% and GSM8K from 10.4\% to 40.7\% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.},
	urldate = {2023-03-07},
	publisher = {arXiv},
	author = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
	month = jan,
	year = {2023},
	note = {arXiv:2205.11916 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\ChekF\\Zotero\\storage\\LMCJ2WZC\\Kojima et al. - 2023 - Large Language Models are Zero-Shot Reasoners.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ChekF\\Zotero\\storage\\9CUSMB93\\2205.html:text/html},
}


@inproceedings{lai_memory_1999,
	title = {Memory sharing predictor: the key to a speculative coherent {DSM}},
	url = {https://ieeexplore.ieee.org/document/765949},
	doi = {10.1109/ISCA.1999.765949},
	shorttitle = {Memory sharing predictor},
	abstract = {Recent research advocates using general message predictors to learn and predict the coherence activity in distributed shared memory ({DSM}). By accurately predicting a message and timely invoking the necessary coherence actions, a {DSM} can hide much of the remote access latency. This paper proposes the Memory Sharing Predictors ({MSPs}), pattern-based predictors that significantly improve prediction accuracy and implementation cost over general message predictors. An {MSP} is based on the key observation that to hide the remote access latency, a predictor must accurately predict only the remote memory accesses (i.e., request messages) and not the subsequent coherence messages invoked by an access. Simulation results indicate that {MSPs} improve prediction accuracy over general message predictors from 81\% to 93\% while requiring less storage overhead. This paper also presents the first design and evaluation for a speculative coherent {DSM} using pattern-based predictors. We identify simple techniques and mechanisms to trigger prediction timely and perform speculation for remote read accesses. Our speculation hardware readily works with a conventional full-map write-invalidate coherence protocol without any modifications. Simulation results indicate that performing speculative read requests alone reduces execution times by 12\% in our shared-memory applications.},
	eventtitle = {Proceedings of the 26th International Symposium on Computer Architecture (Cat. No.99CB36367)},
	pages = {172--183},
	booktitle = {Proceedings of the 26th International Symposium on Computer Architecture (Cat. No.99CB36367)},
	author = {Lai, An-Chow and Falsafi, B.},
	urldate = {2023-11-01},
	date = {1999-05},
	note = {{ISSN}: 1063-6897},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\ChekF\\Zotero\\storage\\MPZWG4KI\\765949.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\ChekF\\Zotero\\storage\\ZV2C7XPU\\Lai and Falsafi - 1999 - Memory sharing predictor the key to a speculative.pdf:application/pdf},
}

@inproceedings{kaxiras_coherence_1999,
	location = {Touluse, France},
	title = {Coherence communication prediction in shared-memory multiprocessors},
	isbn = {978-0-7695-0550-3},
	url = {http://ieeexplore.ieee.org/document/824347/},
	doi = {10.1109/HPCA.2000.824347},
	abstract = {Sharing patterns in shared-memory multiprocessors are the key to performance: uniprocessor latencytolerating techniques such as out-of-order execution and non-blocking caches have proved unable to completely hide the latency of remote memory access. Recently proposed prediction mechanisms accelerate coherence protocols by guessing where data will be used next and forwarding them to potential users before they are requested. Prior work in such shared-memory prediction schemes resulted in address-based and instruction-based predictors. Our work innovates in three areas. First, we present a taxonomy of prediction schemes that includes all previously-proposed prediction schemes in a uniform space. Second, we show how statistical techniques from epidemiological screening and polygraph testing can be applied to better measure the effectiveness of sharing prediction schemes; earlier work had reported only the ratio of incorrect predictions to correct predictions but neglected the ratio of correct predictions to actual sharing. Third, we provide simulation results of the accuracy of a practical subset of the space of schemes in our taxonomy, then analyze which components of each scheme contribute the most to prediction accuracy. Through this process, we discovered prediction schemes more accurate than those previously proposed.},
	eventtitle = {{HPCA}: 6th International Symposium on High-Performance Computer Architecutre},
	pages = {156--167},
	booktitle = {Proceedings Sixth International Symposium on High-Performance Computer Architecture. {HPCA}-6 (Cat. No.{PR}00550)},
	publisher = {{IEEE} Comput. Soc},
	author = {Kaxiras, S. and Young, C.},
	urldate = {2023-10-30},
	date = {1999},
	langid = {english},
	file = {Kaxiras and Young - 1999 - Coherence communication prediction in shared-memor.pdf:C\:\\Users\\ChekF\\Zotero\\storage\\24N72C9B\\Kaxiras and Young - 1999 - Coherence communication prediction in shared-memor.pdf:application/pdf},
}

@inproceedings{leventhal_perceptron_2006,
	title = {Perceptron Based Consumer Prediction in Shared-Memory Multiprocessors},
	url = {https://ieeexplore.ieee.org/abstract/document/4380808},
	doi = {10.1109/ICCD.2006.4380808},
	abstract = {Recent research has shown that forwarding speculative data to other processors before it is requested can improve the performance of multiprocessor systems. The most recent work in speculative data forwarding places all of the processors on a single bus, allowing the data to be forwarded to all of the processors at the same cost as any subset of the processors. Modern multiprocessors however often employ more complex switching networks in which broadcast is expensive. Accurately predicting the consumers of data can be challenging, especially in the case of programs with many shared data structures. Past consumer predictors rely on simple prediction mechanisms, a single table lookup followed by a static mapping of the table values onto a prediction. We make two main contributions in this paper. First, we show how to reduce the design space of consumer predictors to a set of interesting predictors, and how previous consumer predictors can be tuned to expand the range of available performance. Second, we propose a perceptron consumer predictor that dynamically adapts its reaction to the system behavior, and uses more history information than previous consumer predictors. This predictor outperforms the previous predictors by 21\% while using only 1KByte more storage than previous predictors.},
	eventtitle = {2006 International Conference on Computer Design},
	pages = {148--154},
	booktitle = {2006 International Conference on Computer Design},
	author = {Leventhal, Sean and Franklin, Manoj},
	urldate = {2023-11-02},
	date = {2006-10},
	note = {{ISSN}: 1063-6404},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\ChekF\\Zotero\\storage\\CNUB5GDV\\4380808.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\ChekF\\Zotero\\storage\\WKSKFTJ9\\Leventhal and Franklin - 2006 - Perceptron Based Consumer Prediction in Shared-Mem.pdf:application/pdf},
}

@inproceedings{yeh_two_1991,
author = {Yeh, Tse-Yu and Patt, Yale N.},
title = {Two-Level Adaptive Training Branch Prediction},
year = {1991},
isbn = {0897914600},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/123465.123475},
doi = {10.1145/123465.123475},
booktitle = {Proceedings of the 24th Annual International Symposium on Microarchitecture},
pages = {51–61},
numpages = {11},
location = {Albuquerque, New Mexico, Puerto Rico},
series = {MICRO 24}
}



@article{zhan_parsec3.0_2016,
author = {Zhan, Xusheng and Bao, Yungang and Bienia, Christian and Li, Kai},
title = {PARSEC3.0: A Multicore Benchmark Suite with Network Stacks and SPLASH-2X},
year = {2017},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {5},
issn = {0163-5964},
url = {https://doi.org/10.1145/3053277.3053279},
doi = {10.1145/3053277.3053279},
abstract = {Benchmarks play a very important role in accelerating the development and research of CMP. As one of them, the PARSEC suite continues to be updated and revised over and over again so that it can offer better support for researchers. The former versions of PARSEC have enough workloads to evaluate the property of CMP about CPU, cache and memory, but it lacks of applications based on network stack to assess the performance of CMPs in respect of network. In this work, we introduce PARSEC3.0, a new version of PARSEC suite that implements a user-level network stack and generates three network workloads with this stack to cover network domain. We explore the input sets of splash-2 and expand them to multiple scales, a.k.a, splash-2x. We integrate splash-2 and splash-2x into PARSEC framework so that researchers use these benchmark suite conveniently. Finally, we evaluate the u-TCP/IP stack and new network workloads, and analyze the characterizes of splash-2 and splash-2x},
journal = {SIGARCH Comput. Archit. News},
month = {feb},
pages = {1–16},
numpages = {16}
}


